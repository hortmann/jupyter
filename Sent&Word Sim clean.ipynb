{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "immune-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from collections import OrderedDict\n",
    "from typing import Union, List\n",
    "from pandas import ExcelWriter\n",
    "\n",
    "\n",
    "##################################################################\n",
    "\n",
    "def load_corpus_as_sentlist(res_pklfile,jd_pklfile):\n",
    "    import pickle\n",
    "    #load cleaned sentence lists for jd and res\n",
    "    with open(res_pklfile, \"rb\") as handle:\n",
    "        res_pkg=pickle.load(handle)\n",
    "        handle.close()\n",
    "        r=res_pkg['sentences']\n",
    "    with open(jd_pklfile, \"rb\") as handle:\n",
    "        jd_pkg=pickle.load(handle)\n",
    "        handle.close()\n",
    "        j=jd_pkg['sentences']\n",
    "    return(r,j)\n",
    "\n",
    "##################################################################\n",
    "def load_and_embed_sent_transformer(pretrained_modelname):\n",
    "    model = SentenceTransformer(pretrained_modelname)\n",
    "    model.max_seq_length = 256\n",
    "    \n",
    "    \n",
    "    #Compute embedding for both lists\n",
    "    s_emb_r = model.encode(sent_r, convert_to_tensor=True)\n",
    "    s_emb_j = model.encode(sent_j, convert_to_tensor=True)\n",
    "    w_emb_r = model.encode(sent_r, output_value='token_embeddings', convert_to_tensor=True)\n",
    "    w_emb_j = model.encode(sent_j,output_value='token_embeddings', convert_to_tensor=True)\n",
    "\n",
    "\n",
    "    #Compute cosine-similarities for sentences\n",
    "    cos_s = util.pytorch_cos_sim(s_emb_r, s_emb_j)\n",
    "\n",
    "\n",
    "    tf=model.add_tokens([])\n",
    "    voc=tf[0].get_vocab()\n",
    "\n",
    "    with open('tf.pickle','wb') as handle:\n",
    "        pickle.dump(tf[0], handle,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # # loading\n",
    "    with open('tf.pickle', 'rb') as handle:\n",
    "        tf_adpt = pickle.load(handle)\n",
    "        handle.close\n",
    "    return({'s_r':s_emb_r,'s_j':s_emb_j,'w_r':w_emb_r,'w_j':w_emb_j},voc, model,cos_s)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "# converting a list of sentences into a list of words\n",
    "def wordlist_from_sentencelist(corp):\n",
    "    word_list=[]\n",
    "    for s in corp:\n",
    "        new_s=re.sub('[^A-Za-z0-9äöü]+', ' ', s)\n",
    "        word_list.extend(new_s.split())\n",
    "        word_list=list(set(word_list)-set(['','.','-','a','an','in','at','the','up','my','(',')'])) # removing duplicates and '' here\n",
    "        word_list.sort()\n",
    "    return word_list\n",
    "\n",
    "##################################################################\n",
    "\n",
    "# finding all unknown words from word_list in vocabulary\n",
    "def find_ukn_words(word_list,vocab):\n",
    "    ukn_word_list=[]\n",
    "    char_not_allowed=['',',','(',')']\n",
    "    for w in word_list:\n",
    "        for c in char_not_allowed:\n",
    "            w=w.replace(c,'')\n",
    "            if w not in ['','.'] and w not in vocab:\n",
    "                ukn_word_list.append(w)\n",
    "    return ukn_word_list # list of words not found in vocab\n",
    "\n",
    "##################################################################\n",
    "\n",
    "# add the words in word_list to the vocab and resize the model\n",
    "def add_words_to_vocab_and_save(word_list):\n",
    "    tf=model.add_tokens(word_list)\n",
    "    \n",
    "    return tf\n",
    "\n",
    "##################################################################\n",
    "\n",
    "def remove_duplicates(wordlist):\n",
    "    final_list = list(dict.fromkeys(wordlist))\n",
    "    return final_list\n",
    "\n",
    "##################################################################\n",
    "\n",
    "#generates wordlist from list of sentences, cleanS special characters and duplicates and extract words not in vocab\n",
    "def get_clean_wordlist(corp,vocab):\n",
    "    wl=wordlist_from_sentencelist(corp)\n",
    "    unknown_wl=find_ukn_words(wl,vocab)\n",
    "    l=remove_duplicates(unknown_wl)\n",
    "    return l   \n",
    "\n",
    "##################################################################\n",
    "def get_words_with_tensors(word_emb):\n",
    "    #input_ids\n",
    "    wlist=[]\n",
    "    i_id=word_emb['input_ids']\n",
    "    t_emb=word_emb['token_embeddings']\n",
    "    for i in range(0,len(i_id)-1):\n",
    "        for j in range(0,len(i_id[i])-1):\n",
    "            w=list(adpt_voc)[int(i_id[i][j])]\n",
    "            idx=int(i_id[i][j])\n",
    "            v=t_emb[i][j]\n",
    "            if (idx >200):\n",
    "                wlist.append((idx,w,v))\n",
    "    return(wlist)\n",
    "\n",
    "##################################################################\n",
    "\n",
    "def cosim_words(w_r,w_j):\n",
    "    cosim_r=[]\n",
    "    cosim_j=[]\n",
    "    start=time.time()\n",
    "    for r in range(0,len(w_r)):\n",
    "        for j in range(0,len(w_j)):\n",
    "            cosim_j.append(float(util.pytorch_cos_sim(w_r[r][2],w_j[j][2])))\n",
    "        cosim_r.append(cosim_j)\n",
    "        cosim_j=[]\n",
    "    end=time.time()\n",
    "    #print('finished in ', end-start, ' seconds')\n",
    "    return cosim_r\n",
    "    \n",
    "##################################################################    \n",
    "def make_word_panda(w_dict_r,w_dict_j,cosim_r):\n",
    "    res_words=[]\n",
    "    jd_words=[]\n",
    "    for w in w_dict_j:\n",
    "        jd_words.append(w[1])\n",
    "    for x in w_dict_r:\n",
    "        res_words.append(x[1])\n",
    "    PATH='Z:/FILES/OUTPUT/'\n",
    "    df=pd.DataFrame(cosim_r, columns=jd_words, index=res_words)\n",
    "    fname=PATH+'word_sim_'+str(time.time())+'.xlsx'\n",
    "    with pd.ExcelWriter(fname) as writer:  \n",
    "        df.to_excel(writer, sheet_name='CosSim Words')\n",
    "    return df\n",
    "\n",
    "def make_sent_pairs(sent_r,sent_j,cos_s):\n",
    "    #Find the pairs with the highest cosine similarity scores\n",
    "    pairs = []\n",
    "    no=0\n",
    "    for i in range(len(cos_s)):\n",
    "        for k in range(i+1, len(cos_s)):\n",
    "            pairs.append({'pair_no': 0,'index': [i, k], 'score': cos_s[i][k].item(), 'sentence RES':sent_r[i], 'sentence JD':sent_j[k]})\n",
    "\n",
    "    #Sort scores in decreasing order\n",
    "    pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\n",
    "    for j in range(0,len(pairs)-1):\n",
    "        pairs[j]['pair_no']=no\n",
    "        no += 1\n",
    "    \n",
    "    PATH='Z:/FILES/OUTPUT/'\n",
    "    df_spairs=pd.DataFrame(pairs)\n",
    "    fname=PATH+'sent_sim_'+str(time.time())+'.xlsx'\n",
    "    with pd.ExcelWriter(fname) as writer:  \n",
    "        df_spairs.to_excel(writer, sheet_name='CosSim Sent')   \n",
    "    return pairs,df_spairs\n",
    "##################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "extended-screen",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 1.2.0, however, your version is 1.1.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tokens BEFORE:  30522\n",
      "We have added 0 tokens\n",
      "# of tokens AFTER:  30522\n",
      "# of tokens BEFORE:  30522\n",
      "We have added 21 tokens\n",
      "# of tokens AFTER:  30543\n",
      "# of tokens BEFORE:  30543\n",
      "We have added 8 tokens\n",
      "# of tokens AFTER:  30551\n",
      "main finished after  22.186150312423706  seconds\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "# loading corpus as clean lists of sentetnces from pickle files\n",
    "sent_r,sent_j=load_corpus_as_sentlist('res_slist.pkl','jd_slist.pkl')\n",
    "# getting the embeddings for sentences and words and the vocabulary\n",
    "emb,voc, model,cos_s=load_and_embed_sent_transformer(\"paraphrase-MiniLM-L12-v2\")\n",
    "\n",
    "#add unkn words to vocab from both docs\n",
    "clean=get_clean_wordlist(sent_j,voc)\n",
    "adpt_tokenizer,adpt_model,adpt_voc=model.add_tokens(clean)\n",
    "clean=get_clean_wordlist(sent_r,adpt_voc)\n",
    "adpt_tokenizer,adpt_model,adpt_voc=model.add_tokens(clean)\n",
    "\n",
    "#embed words again with extended vocabulary\n",
    "w_emb_r = model.encode(sent_r, output_value='token_embeddings', convert_to_tensor=True)\n",
    "#get a dict with idx, words and tensors\n",
    "w_dict_r=get_words_with_tensors(model.tokenized_list)\n",
    "\n",
    "#the same 2 steps for jd\n",
    "w_emb_j = model.encode(sent_j,output_value='token_embeddings', convert_to_tensor=True)\n",
    "w_dict_j=get_words_with_tensors(model.tokenized_list)\n",
    "\n",
    "w_sim=cosim_words(w_dict_r,w_dict_j)\n",
    "df=make_word_panda(w_dict_r,w_dict_j,cosim_r)\n",
    "\n",
    "pairs,df_spairs=make_sent_pairs(sent_r,sent_j,cos_s)\n",
    "end=time.time()\n",
    "print(\"main finished after \", end-start, \" seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "suburban-robertson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>we</th>\n",
       "      <th>offer</th>\n",
       "      <th>an</th>\n",
       "      <th>interesting</th>\n",
       "      <th>challenge</th>\n",
       "      <th>and</th>\n",
       "      <th>attractive</th>\n",
       "      <th>employment</th>\n",
       "      <th>conditions</th>\n",
       "      <th>in</th>\n",
       "      <th>...</th>\n",
       "      <th>from</th>\n",
       "      <th>eur</th>\n",
       "      <th>deliver</th>\n",
       "      <th>service</th>\n",
       "      <th>enhancements</th>\n",
       "      <th>key</th>\n",
       "      <th>responsibilities</th>\n",
       "      <th>:</th>\n",
       "      <th>itil</th>\n",
       "      <th>certificate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>conducted</th>\n",
       "      <td>0.290504</td>\n",
       "      <td>0.217180</td>\n",
       "      <td>0.021576</td>\n",
       "      <td>0.037805</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.043740</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.081655</td>\n",
       "      <td>0.197000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099950</td>\n",
       "      <td>0.147496</td>\n",
       "      <td>0.270335</td>\n",
       "      <td>0.153928</td>\n",
       "      <td>0.125402</td>\n",
       "      <td>0.058871</td>\n",
       "      <td>0.126191</td>\n",
       "      <td>0.111210</td>\n",
       "      <td>0.140330</td>\n",
       "      <td>0.074244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>an</th>\n",
       "      <td>0.255133</td>\n",
       "      <td>-0.162681</td>\n",
       "      <td>0.656741</td>\n",
       "      <td>-0.022993</td>\n",
       "      <td>-0.016443</td>\n",
       "      <td>0.452546</td>\n",
       "      <td>-0.048022</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>-0.150795</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220735</td>\n",
       "      <td>0.215888</td>\n",
       "      <td>0.164734</td>\n",
       "      <td>0.235080</td>\n",
       "      <td>0.206386</td>\n",
       "      <td>0.031140</td>\n",
       "      <td>0.045120</td>\n",
       "      <td>0.381500</td>\n",
       "      <td>0.260966</td>\n",
       "      <td>0.081294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>effort</th>\n",
       "      <td>0.045881</td>\n",
       "      <td>0.320336</td>\n",
       "      <td>-0.068085</td>\n",
       "      <td>0.270919</td>\n",
       "      <td>0.421351</td>\n",
       "      <td>-0.080165</td>\n",
       "      <td>0.278598</td>\n",
       "      <td>0.153157</td>\n",
       "      <td>0.262735</td>\n",
       "      <td>0.216755</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137024</td>\n",
       "      <td>-0.067426</td>\n",
       "      <td>0.088918</td>\n",
       "      <td>0.134098</td>\n",
       "      <td>-0.040824</td>\n",
       "      <td>0.049422</td>\n",
       "      <td>0.111341</td>\n",
       "      <td>-0.044989</td>\n",
       "      <td>-0.127493</td>\n",
       "      <td>-0.059873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.166413</td>\n",
       "      <td>0.048974</td>\n",
       "      <td>0.286482</td>\n",
       "      <td>0.082190</td>\n",
       "      <td>0.151251</td>\n",
       "      <td>0.270185</td>\n",
       "      <td>0.101064</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.073792</td>\n",
       "      <td>0.330357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.129247</td>\n",
       "      <td>0.106327</td>\n",
       "      <td>0.282467</td>\n",
       "      <td>0.307498</td>\n",
       "      <td>0.220401</td>\n",
       "      <td>0.106019</td>\n",
       "      <td>0.186611</td>\n",
       "      <td>0.272176</td>\n",
       "      <td>0.123253</td>\n",
       "      <td>0.111432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>migrate</th>\n",
       "      <td>0.010095</td>\n",
       "      <td>0.069037</td>\n",
       "      <td>-0.004819</td>\n",
       "      <td>0.080557</td>\n",
       "      <td>0.142928</td>\n",
       "      <td>-0.019021</td>\n",
       "      <td>0.156295</td>\n",
       "      <td>0.088632</td>\n",
       "      <td>0.111894</td>\n",
       "      <td>0.184071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.252025</td>\n",
       "      <td>0.026736</td>\n",
       "      <td>0.133403</td>\n",
       "      <td>0.022892</td>\n",
       "      <td>-0.047117</td>\n",
       "      <td>-0.085377</td>\n",
       "      <td>-0.002894</td>\n",
       "      <td>0.056167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>austria</th>\n",
       "      <td>-0.028889</td>\n",
       "      <td>0.060457</td>\n",
       "      <td>-0.004254</td>\n",
       "      <td>0.084854</td>\n",
       "      <td>-0.009719</td>\n",
       "      <td>0.013197</td>\n",
       "      <td>0.111491</td>\n",
       "      <td>0.068423</td>\n",
       "      <td>0.084457</td>\n",
       "      <td>0.065421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040543</td>\n",
       "      <td>0.080879</td>\n",
       "      <td>-0.068190</td>\n",
       "      <td>-0.093890</td>\n",
       "      <td>0.011948</td>\n",
       "      <td>0.124280</td>\n",
       "      <td>0.040517</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.103760</td>\n",
       "      <td>0.013448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.187649</td>\n",
       "      <td>-0.171762</td>\n",
       "      <td>0.516886</td>\n",
       "      <td>-0.096962</td>\n",
       "      <td>-0.046474</td>\n",
       "      <td>0.419285</td>\n",
       "      <td>-0.119960</td>\n",
       "      <td>-0.032790</td>\n",
       "      <td>-0.199589</td>\n",
       "      <td>-0.060601</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143213</td>\n",
       "      <td>0.124577</td>\n",
       "      <td>0.119516</td>\n",
       "      <td>0.113384</td>\n",
       "      <td>0.169266</td>\n",
       "      <td>-0.017603</td>\n",
       "      <td>0.079484</td>\n",
       "      <td>0.366957</td>\n",
       "      <td>0.269492</td>\n",
       "      <td>0.211395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professional</th>\n",
       "      <td>0.056774</td>\n",
       "      <td>-0.034594</td>\n",
       "      <td>0.261446</td>\n",
       "      <td>0.028801</td>\n",
       "      <td>0.022207</td>\n",
       "      <td>0.224271</td>\n",
       "      <td>0.064881</td>\n",
       "      <td>0.149080</td>\n",
       "      <td>-0.046366</td>\n",
       "      <td>0.119094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298342</td>\n",
       "      <td>0.068160</td>\n",
       "      <td>0.254476</td>\n",
       "      <td>0.268654</td>\n",
       "      <td>0.194699</td>\n",
       "      <td>0.175701</td>\n",
       "      <td>0.346341</td>\n",
       "      <td>0.430766</td>\n",
       "      <td>0.220726</td>\n",
       "      <td>0.402097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experience</th>\n",
       "      <td>0.045517</td>\n",
       "      <td>0.144092</td>\n",
       "      <td>0.182948</td>\n",
       "      <td>0.139601</td>\n",
       "      <td>0.190277</td>\n",
       "      <td>0.258731</td>\n",
       "      <td>0.149759</td>\n",
       "      <td>0.154029</td>\n",
       "      <td>0.180029</td>\n",
       "      <td>0.272516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197197</td>\n",
       "      <td>0.069972</td>\n",
       "      <td>0.172443</td>\n",
       "      <td>0.256890</td>\n",
       "      <td>0.136585</td>\n",
       "      <td>0.104162</td>\n",
       "      <td>0.361729</td>\n",
       "      <td>0.399619</td>\n",
       "      <td>0.131578</td>\n",
       "      <td>0.298068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>0.052036</td>\n",
       "      <td>0.060167</td>\n",
       "      <td>0.158455</td>\n",
       "      <td>0.096788</td>\n",
       "      <td>0.063284</td>\n",
       "      <td>0.207788</td>\n",
       "      <td>0.045077</td>\n",
       "      <td>0.112648</td>\n",
       "      <td>0.043738</td>\n",
       "      <td>0.159696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165044</td>\n",
       "      <td>-0.028339</td>\n",
       "      <td>0.215647</td>\n",
       "      <td>0.145371</td>\n",
       "      <td>0.144463</td>\n",
       "      <td>0.129238</td>\n",
       "      <td>0.254799</td>\n",
       "      <td>0.334306</td>\n",
       "      <td>0.112249</td>\n",
       "      <td>0.241775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411 rows × 424 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    we     offer        an  interesting  challenge       and  \\\n",
       "conducted     0.290504  0.217180  0.021576     0.037805   0.014925  0.043740   \n",
       "an            0.255133 -0.162681  0.656741    -0.022993  -0.016443  0.452546   \n",
       "effort        0.045881  0.320336 -0.068085     0.270919   0.421351 -0.080165   \n",
       "to            0.166413  0.048974  0.286482     0.082190   0.151251  0.270185   \n",
       "migrate       0.010095  0.069037 -0.004819     0.080557   0.142928 -0.019021   \n",
       "...                ...       ...       ...          ...        ...       ...   \n",
       "austria      -0.028889  0.060457 -0.004254     0.084854  -0.009719  0.013197   \n",
       ".             0.187649 -0.171762  0.516886    -0.096962  -0.046474  0.419285   \n",
       "professional  0.056774 -0.034594  0.261446     0.028801   0.022207  0.224271   \n",
       "experience    0.045517  0.144092  0.182948     0.139601   0.190277  0.258731   \n",
       "education     0.052036  0.060167  0.158455     0.096788   0.063284  0.207788   \n",
       "\n",
       "              attractive  employment  conditions        in  ...      from  \\\n",
       "conducted       0.026555    0.025199    0.081655  0.197000  ...  0.099950   \n",
       "an             -0.048022    0.033652   -0.150795  0.007098  ...  0.220735   \n",
       "effort          0.278598    0.153157    0.262735  0.216755  ... -0.137024   \n",
       "to              0.101064    0.036500    0.073792  0.330357  ...  0.129247   \n",
       "migrate         0.156295    0.088632    0.111894  0.184071  ...  0.001143   \n",
       "...                  ...         ...         ...       ...  ...       ...   \n",
       "austria         0.111491    0.068423    0.084457  0.065421  ...  0.040543   \n",
       ".              -0.119960   -0.032790   -0.199589 -0.060601  ...  0.143213   \n",
       "professional    0.064881    0.149080   -0.046366  0.119094  ...  0.298342   \n",
       "experience      0.149759    0.154029    0.180029  0.272516  ...  0.197197   \n",
       "education       0.045077    0.112648    0.043738  0.159696  ...  0.165044   \n",
       "\n",
       "                   eur   deliver   service  enhancements       key  \\\n",
       "conducted     0.147496  0.270335  0.153928      0.125402  0.058871   \n",
       "an            0.215888  0.164734  0.235080      0.206386  0.031140   \n",
       "effort       -0.067426  0.088918  0.134098     -0.040824  0.049422   \n",
       "to            0.106327  0.282467  0.307498      0.220401  0.106019   \n",
       "migrate       0.048100  0.252025  0.026736      0.133403  0.022892   \n",
       "...                ...       ...       ...           ...       ...   \n",
       "austria       0.080879 -0.068190 -0.093890      0.011948  0.124280   \n",
       ".             0.124577  0.119516  0.113384      0.169266 -0.017603   \n",
       "professional  0.068160  0.254476  0.268654      0.194699  0.175701   \n",
       "experience    0.069972  0.172443  0.256890      0.136585  0.104162   \n",
       "education    -0.028339  0.215647  0.145371      0.144463  0.129238   \n",
       "\n",
       "              responsibilities         :      itil  certificate  \n",
       "conducted             0.126191  0.111210  0.140330     0.074244  \n",
       "an                    0.045120  0.381500  0.260966     0.081294  \n",
       "effort                0.111341 -0.044989 -0.127493    -0.059873  \n",
       "to                    0.186611  0.272176  0.123253     0.111432  \n",
       "migrate              -0.047117 -0.085377 -0.002894     0.056167  \n",
       "...                        ...       ...       ...          ...  \n",
       "austria               0.040517  0.005090  0.103760     0.013448  \n",
       ".                     0.079484  0.366957  0.269492     0.211395  \n",
       "professional          0.346341  0.430766  0.220726     0.402097  \n",
       "experience            0.361729  0.399619  0.131578     0.298068  \n",
       "education             0.254799  0.334306  0.112249     0.241775  \n",
       "\n",
       "[411 rows x 424 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "tropical-kitty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_no</th>\n",
       "      <th>index</th>\n",
       "      <th>score</th>\n",
       "      <th>sentence RES</th>\n",
       "      <th>sentence JD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[6, 7]</td>\n",
       "      <td>0.796949</td>\n",
       "      <td>set up the company service desk and assess tea...</td>\n",
       "      <td>sets up and maintain service desk, including m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[6, 20]</td>\n",
       "      <td>0.750100</td>\n",
       "      <td>set up the company service desk and assess tea...</td>\n",
       "      <td>experience in setting up and running a service...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[1, 19]</td>\n",
       "      <td>0.730559</td>\n",
       "      <td>experienced it service lead and global it cons...</td>\n",
       "      <td>to  years of it service management work experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 4]</td>\n",
       "      <td>0.721961</td>\n",
       "      <td>it service lead and consultant</td>\n",
       "      <td>teamlead it service management  .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 19]</td>\n",
       "      <td>0.663157</td>\n",
       "      <td>it service lead and consultant</td>\n",
       "      <td>to  years of it service management work experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>491</td>\n",
       "      <td>[16, 25]</td>\n",
       "      <td>-0.023737</td>\n",
       "      <td>transitioning an in-sourced business unit from...</td>\n",
       "      <td>language skills: english fluent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>492</td>\n",
       "      <td>[5, 25]</td>\n",
       "      <td>-0.025321</td>\n",
       "      <td>modis, vienna, austria .</td>\n",
       "      <td>language skills: english fluent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>493</td>\n",
       "      <td>[7, 29]</td>\n",
       "      <td>-0.045077</td>\n",
       "      <td>in charge of developing and delivering strateg...</td>\n",
       "      <td>gross monthly salary: from eur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>494</td>\n",
       "      <td>[5, 13]</td>\n",
       "      <td>-0.052839</td>\n",
       "      <td>modis, vienna, austria .</td>\n",
       "      <td>monitors department issues and client complain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0</td>\n",
       "      <td>[13, 25]</td>\n",
       "      <td>-0.088033</td>\n",
       "      <td>zurich insurance company ltd, vienna, austria</td>\n",
       "      <td>language skills: english fluent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pair_no     index     score  \\\n",
       "0          0    [6, 7]  0.796949   \n",
       "1          1   [6, 20]  0.750100   \n",
       "2          2   [1, 19]  0.730559   \n",
       "3          3    [0, 4]  0.721961   \n",
       "4          4   [0, 19]  0.663157   \n",
       "..       ...       ...       ...   \n",
       "491      491  [16, 25] -0.023737   \n",
       "492      492   [5, 25] -0.025321   \n",
       "493      493   [7, 29] -0.045077   \n",
       "494      494   [5, 13] -0.052839   \n",
       "495        0  [13, 25] -0.088033   \n",
       "\n",
       "                                          sentence RES  \\\n",
       "0    set up the company service desk and assess tea...   \n",
       "1    set up the company service desk and assess tea...   \n",
       "2    experienced it service lead and global it cons...   \n",
       "3                       it service lead and consultant   \n",
       "4                       it service lead and consultant   \n",
       "..                                                 ...   \n",
       "491  transitioning an in-sourced business unit from...   \n",
       "492                           modis, vienna, austria .   \n",
       "493  in charge of developing and delivering strateg...   \n",
       "494                           modis, vienna, austria .   \n",
       "495      zurich insurance company ltd, vienna, austria   \n",
       "\n",
       "                                           sentence JD  \n",
       "0    sets up and maintain service desk, including m...  \n",
       "1    experience in setting up and running a service...  \n",
       "2    to  years of it service management work experi...  \n",
       "3                    teamlead it service management  .  \n",
       "4    to  years of it service management work experi...  \n",
       "..                                                 ...  \n",
       "491                    language skills: english fluent  \n",
       "492                    language skills: english fluent  \n",
       "493                     gross monthly salary: from eur  \n",
       "494  monitors department issues and client complain...  \n",
       "495                    language skills: english fluent  \n",
       "\n",
       "[496 rows x 5 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
